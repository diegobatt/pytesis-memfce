

A lo largo del presente trabajo se buscó realizar una introducción teórica de los métodos estadísticos existentes aplicados al análisis topológico de datos @ConfidenceSetsForPersistenceDiagrams, en particular dentro del área de homología persistente, con el objetivo de inferir la topología subyacente, descrita a partir de los agujeros de primer orden que la misma presente, de un conjunto de datos. En la @sec-intro se introdujo cómo la función de distancia elegida cumple un rol fundamental en esta tarea, por lo que en la @sec-intro-fermat se presentó la Distancia de Fermat, presentándose ésta como una alternativa a la distancia Euclídea, con la característica de que logra codificar en las distancias de los puntos entre sí de un conjunto de datos de forma más apropiada la topología subyacente de los mismos (@IntrinsicPersistentHomologyFermatDistance, @PatuFermatDistance).
En la @sec-methods se detalló cómo esta distancia se utilizó para construir diagramas de persistencia, con sus respectivos intervalos de confianza, de los distintos conjuntos de datos introducidos, tanto sintéticos como reales. Los resultados obtenidos a lo largo de la @sec-results muestran cómo se comparan los resultados del método de Fermat contra los obtenidos según los métodos discutidos en la bibliografía @ConfidenceSetsForPersistenceDiagrams, los cuales llamamos método Euclídeo y KDE.
En particular, en la @sec-results-synth, donde se analizaron los conjuntos de datos sintéticos, se observan las primeras diferencias y similitudes entre los distintos métodos.

Resulta importante destacar que para los resultados expuestos a lo largo de todo el trabajo, distintos valores de hiperparámetros fueron probados, conservando el conjunto de valores que mejores resultados obtenía en las pruebas realizadas para cada uno de los métodos. Una línea de trabajo interesante que se desprende de los resultados obtenidos es el impacto de los distintos valores de hiperparámetros en los mismos. Esta experimentación requiere de un muy alto poder computacional, dada la complejidad de los algoritmos utilizados para construir los diagramas de persistencia y teniendo en cuenta que la construcción de intervalos de confianza sobre los mismos requiere realizar esta operación decenas de veces. Particularmente, para el caso del  hiperparámetro $\alpha$ utilizado como potencia en el cálculo de la distancia de Fermat, el mismo demostró ser bastante robusto ya que su valor más intuitivo $\alpha = 2$ logra buenos resultados, encontrándose como resultado heurístico que un rango de valores entre $1.8$ y $2.7$ para $\alpha$ funcionan también de buena manera. Teniendo en cuenta la interpretación de este hiperparámetro y su impacto directo en el cálculo de la distancia, este rango de valores funcionales resulta muy amplio e intuitivo.

Para todos los conjuntos de datos sintéticos que presentan un agujero se observan resultados consistentes, en los que Fermat logra determinar correctamente la presencia del mismo para la totalidad de los *datasets*, mientras que KDE y Euclídeo fallan para el conjunto de Anteojos, detectando dos agujeros, pero lo logran para las circunferencias. Problemas discutidos en la bibliografía (@ConfidenceSetsForPersistenceDiagrams, @FootballRobustDataset) como son la no uniformidad en la densidad de muestreo sobre la topología (que impacta principalmente en el la circunferencia con muestreo Gaussiano) no resultan ser problemáticos para los métodos estudiados.

Los conjuntos de datos fueron perturbados con ruido gaussiano y datos atípicos, para el caso de ruido, los resultados se mantienen prácticamente análogos al conjunto original, como se observa en la @tbl-power-table-noise. Resultaría interesante como continuación de este trabajo analizar cómo estos resultados varían en función de la varianza del ruido agregado.

Distinto es el caso de datos atípicos agregados, ya que para los modelos basados en distancia, que reconstruyen la topología a partir de complejos simpliciales, una muestra bien ubicada es suficiente para cerrar un ciclo de los mismos, distorsionando la topología recuperada. Así se evidencia para las simulaciones realizadas en la @tbl-power-table-outliers, en donde puede verse como KDE es el único no afectado, ya que tanto Fermat como Euclídeo logran detectar en entre un 16% (Fermat) y 20% (Euclídeo) una topología distinta la verdadera, con mayor o menor cantidad de agujeros. Merece una mención especial la capacidad del método de Fermat de detectar de forma significativa para el conjunto de datos con muestras atípicas que la topología presenta más componentes conexas, teniendo en cuenta que para todos nuestros conjuntos de datos se espera una única componente de grado cero. Más aún, en todos los casos analizados, la cantidad de componentes conexas coincide con la cantidad de datos atípicos, lo que motiva un trabajo futuro de estos métodos como mecanismo de detección de muestras atípicas.

Si bien el método KDE es el único no afectado por la presencia de datos atípicos para los conjuntos de datos con agujeros, esto toma una especial perspectiva cuando se analiza el conjunto de datos del círculo con densidad dependiente del radio. En @ConfidenceSetsForPersistenceDiagrams se establece que la distorsión del espacio original que realiza KDE mantiene la topología principal subyacente a la vez que hace al método más robusto al ruido, suavizando ruido y datos atípicos, pero con este conjunto de datos se observa que el intervalo de confianza obtenido para un nivel de 95% no parece ser consistente con detectar agujeros en solo el 5% de las corridas, más aún, el método es incapaz de aceptar la hipótesis nula en la mitad de las corridas, sabiendo que esta es verdadera. Esto, junto con la dificultad a la hora de extender este método a dimensiones superiores por cuestiones computacionales, representan dos desventajas muy claras que el método propuesto en @ConfidenceSetsForPersistenceDiagrams tienen frente al uso de Fermat como métrica.

A la hora de analizar conjuntos de datos reales (@sec-results-real-data), es fácil verificar uno de los problemas que la bibliografía menciona sobre KDE @FootballRobustDataset. Al no tener nuestro conjunto de datos agujeros sin la presencia de un borde, debe adicionarse uno de forma artificial, pero la densidad de muestras que se le asignen a este borde determinará en buena medida si KDE considera o no a estas muestras como parte de la topología subyacente a analizar, ya que el suavizado propio de este método podría simplemente obviar. Como contraparte de esta necesidad está la creciente carga computacional de agregar más puntos al conjunto de datos original. Para los valores utilizados, los resultados de KDE muestran nuevamente una debilidad de este método, consistente a lo obtenido en @FootballRobustDataset. Por su parte, el método de Fermat logra detectar una cantidad de agujeros significativos muy similar a lo que arroja una inspección visual del problema, por lo que se considera que el método es efectivo para este caso de uso, más aún, la similitud en los resultados obtenidos para los diagramas de persistencia entre jugadores diferentes que comparten posición resulta muy interesante. El método Euclídeo por su parte logra un diagrama de persistencia saludable pero no logra una detección de agujeros significativos que coincidan apropiadamente con la inspección visual de los conjuntos de datos.
Resulta una interesante línea de investigación analizar el comportamiento de estos métodos para otro tipo de conjuntos de datos reales.

A partir de todos los resultados obtenidos se concluye entonces que utilizar los métodos basados en distancias con Fermat como medida de las mismas logra solucionar muchos de los problemas que presenta el utilizar distancia Euclídea o el método de ventanas de densidad para analizar una variedad de conjuntos de datos.
